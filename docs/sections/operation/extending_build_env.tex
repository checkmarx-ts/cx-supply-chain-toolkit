\chapter{Integrating \scaresolver into Build Environments}\label{chap:ext_build_env}

If you've determined that your dependency resolution scanning does not execute properly
in a \hyperref[sssec:remote_environment]{\textit{Remote Execution Environment}}, \scaresolver
is the solution for executing the dependency resolution in an environment you have defined.
The Checkmarx \scaresolver is the scan command line tool that invokes 
dependency scans locally.  It can be invoked directly or as part of another
tool such as \cxflow, the \cxonecli, or other Checkmarx plugins.

\cxsca is a standalone product that provides a portal that manages supply chain vulnerability scans.  This is
typically used in combination with the \cxsast product to provide both static analysis and supply-chain vulnerability
scans.  \scaresolver can communicate directly with \cxsca to upload data from the locally executed supply-chain
scan, which would have presumably executed in your customized build environment.

\cxone is a product that combines multiple scan types, including supply-chain vulnerability scans, into a single
view.  Scans are typically invoked using the \cxonecli where the type of scan to invoke is defined as part
of the CLI execution parameters.  By default, the \cxonecli will use the 
\hyperref[sssec:remote_environment]{\textit{Remote Execution Environment}} to perform the supply-chain
vulnerability scan.  The \cxonecli can be given a path to the \scaresolver executable to allow
the supply-chain vulnerability scan to execute in your customized build environment.  The \cxonecli the uploads
the results to \cxone for final analysis that reports any potentially vulnerable packages.


\noindent\\This section documents methods for integrating the execution of \scaresolver into your
build environment.


\section{Deploying on a Build Agent}

When builds are executed on a specific build agent (a.k.a. "The Build Box"),
the invocation of \scaresolver will typically be scripted to execute
within a pipeline stage.  In this scenario, the \scaresolver should be
installed on all build agents that will run pipelines that have scripted
a supply chain scan invocation.  This is the most simple scenario and applies
when using \scaresolver with \cxsca or \cxone.

Note that in this scenario, updates to \scaresolver will need to be
periodically installed.  There are likely several tools on the build agent
that need periodic update; \scaresolver will simply be one additional
tool that requires occasional update.

\section{Modifying a Containerized Build Environment}

A variation of the build agent deployment is found when containerized
build environments are defined as the execution environment for
pipeline stages.  The supply chain scan invocation is scripted similar
to how it would be invoked when using a build agent.  The main difference
is that the pipeline stage is configured to execute inside a specified
container image.  The container image contains all the tools required
to successfully build the software, which means that an accurate
dependency tree can also be generated if the \scaresolver is invoked
in that build environment.

In this integration scenario, the container definition can be modified to install \scaresolver
as part of the container build.  The pipeline scripting running in the container
will invoke \scaresolver as it would invoke any other tool in the scripted
build steps.  

\section{Containerized Build Environment Extension}\label{sec:extending_environment}

Often it is difficult to modify the build agent installation or the build
container definition to add \scaresolver.  It may also present some
difficulties in deployment for some pipeline architectures.  Another option
is to create new container instances that derive from already defined build
environments.  This has several advantages:

\begin{itemize}
    \item No instability can be introduced into known-stable build environments.
    \item The \scaresolver updates can be applied without modifying any 
    containerized build environments.
    \item Deployment of updates is a simple rebuild of the extended containers.
    \item The \cxtoolkit provides a way to generate the extended build 
    environment image for build images running on popular Linux distributions.
    \item The image created with the \cxtoolkit also provides some isolation of 
    dependency resolution activities to avoid some attacks associated with 
    malware embedded in typo-squatted packages.
\end{itemize}

\noindent\\The "build-environment" components can be obtained from the \cxtoolkitpath{releases/latest}{Releases}.


\subsection{Creating Extended Images}

The build-environment components contains a \texttt{Dockerfile} is multi-stage where stage names
specify the correct variation of Linux\footnote{There is currently no support for Windows base images.}
in the base image.  The stage names are intended to align 
with popular base images used to create build environments.  The \texttt{Dockerfile} stages
execute the commands specific to the Linux OS distribution of the base image to properly configure
\scaresolver.

Any image that can be pulled from the public Docker Hub or a private docker registry connected via 
\texttt{docker login} can be defined as the base image.  If the wrong or incompatible stage is specified, 
the container build will fail. To see the base image Linux distribution, one possible method would be
to view the \texttt{/etc/os-release} file found in the base image.  This can be done by executing the following
command:

\noindent\\\\\texttt{docker run ----rm -it ----entrypoint=cat <base image tag> /etc/os-release}


\noindent\\As an example, determining the Linux variation for the \texttt{gradle:latest} image can
be performed with the following command:

\noindent\\\\\texttt{docker run --rm -it --entrypoint=cat gradle:latest /etc/os-release}

\noindent\\The output of \texttt{/etc/os-release} reveals that the Linux variation is Ubuntu, which is a derivative
of Debian.\\\\

\begin{code}{Output of "cat /etc/os-release" from gradle:latest}{}{}
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
\end{code}



\subsection{How to Build an Extended Image}

Building the extended image is done via a \texttt{docker build}\footnote{These instructions can likely be adapted to other container build tools.  Docker is used here since it is
the most commonly used container toolkit at the time this manual was written.} command
using the \texttt{Dockerfile} provided in the \cxtoolkit build-environment.
Docker \href{https://docs.docker.com/build/guide/build-args/}{build arguments} control how the extended image build
is performed.  The most important argument is the \hyperref[sec:BASE]{\textbf{BASE}} build argument.  Other build
arguments are available; details of the available arguments can be found in Appendix \ref{chap:build_args}.



\noindent\\Example build commands:

\begin{code}{Extending Gradle 8.0 Alpine with JDK19}{[with Entrypoint]}{}
docker build -t <your tag> --build-arg BASE=gradle:8.0-jdk19-alpine \
    --target=resolver-alpine .
\end{code}

\begin{code}{Extending Gradle 8.0 Alpine with JDK19}{[without Entrypoint]}{}
docker build -t <your tag> --build-arg BASE=gradle:8.0-jdk19-alpine \
    --target=resolver-alpine-bare .
\end{code}

\begin{code}{Extending Node 19 Alpine}{[with Entrypoint]}{}
docker build -t <your tag> --build-arg BASE=node:19-alpine \
    --target=resolver-alpine .
\end{code}

\begin{code}{Extending Node 19 Alpine}{[without Entrypoint]}{}
docker build -t <your tag> --build-arg BASE=node:19-alpine \
    --target=resolver-alpine-bare .
\end{code}
    
\begin{code}{Extending Node 19 Buster (Debian)}{[with Entrypoint]}{}
    docker build -t <your tag> --build-arg BASE=node:19-buster \
        --target=resolver-debian .
\end{code}

\begin{code}{Extending Node 19 Buster (Debian)}{[without Entrypoint]}{}
docker build -t <your tag> --build-arg BASE=node:19-buster \
    --target=resolver-debian-bare .
\end{code}

\subsection{Extended Image Build Customizations}

There are sub-directories in the build-environment toolkit that are used as part of Building
the extended image.  Items can be add or modified in these directories as appropriate.

\subsubsection{CA Certificates}

The \texttt{cacerts} directory contains Amazon AWS Root CA certs that are used as the CA 
certificate for Checkmarx services.  You can add your own PEM encoded CA certificates in this
directory and it will be included as a trusted CA by the image.

If desired, you can remove the AWS CA files as long as there is at least one PEM
encoded certificate left in the directory during image build.

\subsubsection{\scaresolver Configuration YAML}

The \texttt{default-config} folder contains the \texttt{Configuration.yml} file with a default
configuration for \scaresolver.  It is possible to modify the default configuration so that
common parameter values are not needed to be provided for every invocation of \scaresolver.

\subsection{Dockerfile Targets with Entrypoints}\label{ssec:entrypoint_targets}

If you want to invoke \scaresolver in the extended image in the same way it is invoked from the command
line if it were locally installed, use the entrypoint targets.  If you intend to use the
extended images with the \cxtoolkit 
tools for \hyperref[chap:build_env_affinity]{webhook} scan workflows, extended
images with entrypoints are required.  The current targets that build the extended
image with an entry point are:

\begin{itemize}
    \item \texttt{resolver-alpine}
    \item \texttt{resolver-debian}
    \item \texttt{resolver-redhat}
    \item \texttt{resolver-amazon}
\end{itemize}


\subsection{Bare Dockerfile Targets}\label{ssec:bare_targets}

Containers built with bare targets have no entrypoint and run as root.  
Some CI/CD pipelines will need the ability to execute environment
configuraton commands as root before the stage is executed.  Some CI/CD pipelines
will allow the entrypoint to be overridden and will successfully execute the stage
in the container image.  The Azure Devops pipeline, for example, is not compatible
with images that define an entrypoint.  If your CI/CD pipeline needs a container image without 
an entrypoint, these targets will
produce extended images without an entrypoint:

\begin{itemize}
    \item \texttt{resolver-alpine-bare}
    \item \texttt{resolver-debian-bare}
    \item \texttt{resolver-redhat-bare}
    \item \texttt{resolver-amazon-bare}
\end{itemize}


\section{Extended Containers and Execution Sandboxing}

Building extended images with entrypoint targets will "sandbox" \scaresolver execution 
to the extent possible. This is to allow \scaresolver to execute a dependency resolution
while minimizing the risk of detonating malware payloads found in untrusted build scripts or 
package installation scripts. This is not universally a problem with all dependencies, but 
there is always the possibility for malware delivery via open-source packages. 

\noindent\\In terms of sandboxing, the extended images with entrypoints perform the following sandboxing
activities:

\begin{itemize}
    \item The local user executing the scan has limited privileges.
    \item Code for scanning is provided in a read-only volume map.
    This blocks dependencies that execute code-modifying malware from mutating scanned code.
    \item Output volume maps are write-only, preventing the vulnerability reports and logs
    from being exfiltrated as part of exploitable vulnerability intelligence gathering activities.
\end{itemize}

\noindent\\Nothing is foolproof; don't expect that using this container alone hardens your build
environment.  A threat modeling exercise should be undertaken to understand
if there are other infrastructure changes needed to properly control what is executed in your
build environments.

\subsection{Invoking the Sandbox Container CLI Style}\label{ssec:invoking_cli}

The extended container can invoke \scaresolver or \cxonecli in the same way each CLI tool would be
invoked locally.  Since the container is not executing locally, the code artifacts under scan
need to be mapped to paths inside the container.  Table \ref{table:volume_maps} shows the
container paths where it is appropriate to map volumes during container execution.  

While it is possible
to define your own local container paths for mapping, the paths in Table \ref{table:volume_maps}
have been configured with the container's execution user's permissions set to limit the ability to
interact with the paths as appropriate for the path's purpose.  The \texttt{/sandbox/input} path,
for example, is read-only to prevent modification of the code under scan.

Extended containers with no entrypoint (e.g. the "bare" targets) have the same permissions set
as would be used by extended containers with an entrypoint.  The no-entrypoint targets, however, run
as \texttt{root} which makes the permissions irrelevant.  It is possible to execute the CLI tool as
the sandbox user if desired.  Refer to Appendix \ref{chap:build_args} to see how to detect the user id
of the sandbox user.



\begin{table}[h]
    \caption{Container Volume Mapping Paths}\label{table:volume_maps}      
    \begin{tabularx}{\textwidth}{lcl}
        \toprule
        \textbf{Container Directory} & \textbf{Required} & \textbf{Description}\\
        \midrule
        \texttt{/sandbox/scalogs} & N & \makecell[l]{Used to write \scaresolver logs.}\\
        \midrule
        \texttt{/sandbox/input} & Y & \makecell[l]{This is where the input should be mapped\\
        for \scaresolver inputs.}\\
        \midrule
        \texttt{/sandbox/output} & Y & \makecell[l]{This is the directory where \scaresolver\\
        results files will be written.}\\
        \midrule
        \texttt{/sandbox/report} & Y & \makecell[l]{This is the directory where \scaresolver\\
        report will be written.}\\
        \bottomrule
    \end{tabularx}
\end{table}



\subsubsection{Invoking SCA Resolver}

Extended images with entrypoints can be invoked to use \scaresolver the same way
it would be invoked from the command line with a local install.  Any of the 
\scaresolver
\href{https://checkmarx.com/resource/documents/en/34965-132888-checkmarx-sca-resolver-configuration-arguments.html}{command line arguments}
are passed to the container.  A typical execution is shown below:


\begin{code}{Extended Container Typical CLI Invocation}{[SCA Resolver]}{}
docker run --rm -it \
    -v /my-log-path:/sandbox/scalogs \
    -v .:/sandbox/input \
    -v ./sca-results:/sandbox/output \
    my-container-tag \
    --logs-path /sandbox/scalogs \
    --resolver-result-path /sandbox/output \
    --scan-path /sandbox/input \ 
    <other SCAResolver args...>
\end{code}

Note that options passed to \scaresolver that indicate where to place output should be provided 
with paths prefixed with \texttt{/sandbox} corresponding to the local container paths in 
Table \ref{table:volume_maps}.

\subsubsection{Invoking CxOne CLI with SCA Resolver}

Invoking \scaresolver as a CLI with the extended container is performed by default with the entrypoint
of the extended container.  Using \texttt{cxone} as the first parameter to the extended container will
execute the \cxonecli as if it were invoked from the command line with a local install. A typical
execution is shown below:\\


\begin{code}{Extended Container Typical CLI Invocation}{[CxOne CLI with SCA Resolver]}{}
    docker run --rm -it \
        -v /my-log-path:/sandbox/scalogs \
        -v .:/sandbox/input \
        -v ./sca-results:/sandbox/output \
        my-container-tag \
        cxone \
        scan create \
        --output-path /sandbox/output \
        --sca-resolver /sandbox/resolver/ScaResolver \
        -s /sandbox/input \
        <other CxOne CLI args...>
\end{code}

Note that the \texttt{--sca-resolver} parameter is the container local path where the \scaresolver
executable can be found.
